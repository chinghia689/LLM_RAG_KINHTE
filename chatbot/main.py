"""
Main entry point cho Chatbot RAG v·ªõi Energy Distance Retriever.

M·ªôt ·ª©ng d·ª•ng chatbot s·ª≠ d·ª•ng:
- Vector Database (Chroma) v·ªõi embeddings ti·∫øng Vi·ªát
- Energy-Based Distance cho retrieval n√¢ng cao
- LangGraph cho workflow x·ª≠ l√Ω
- LLM (OpenAI, Gemini, local Ollama, etc.) ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

load_dotenv()

# Th√™m parent folder v√†o path
sys.path.insert(0, str(Path(__file__).parent.parent))

from chatbot.utils.llm import LLM
from chatbot.services.files_rag_chat_agent import FilesChatAgent
from chatbot.utils.graph_state import GraphState


DIR_ROOT = Path(__file__).parent.parent
class ChatbotRunner:
    """
    Runner ch√≠nh ƒë·ªÉ ch·∫°y chatbot ·ª©ng d·ª•ng.
    """

    def __init__(self, path_vector_store, llm_provider="gemini"):
        """
        Kh·ªüi t·∫°o Chatbot.

        Args:
            path_vector_store (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn vector store (VD: './chroma_cosine')
            llm_provider (str): Lo·∫°i LLM (openai, gemini, local, grok). Default: gemini
        """
        self.path_vector_store = path_vector_store
        self.llm_provider = llm_provider

        # Kh·ªüi t·∫°o LLM
        llm_handler = LLM()
        self.llm = llm_handler.get_llm(llm_provider)

        # Kh·ªüi t·∫°o Chatbot Agent
        self.agent = FilesChatAgent(
            llm_model=self.llm,
            path_vector_store=path_vector_store
        )

        # X√¢y d·ª±ng workflow
        self.workflow = self.agent.get_workflow()
        self.compiled_workflow = self.workflow.compile()

    def answer_question(self, question: str, prompt: str = None) -> str:
        """
        Tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.

        Args:
            question (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
            prompt (str, optional): Custom prompt h·ªá th·ªëng. N·∫øu None, d√πng m·∫∑c ƒë·ªãnh.

        Returns:
            str: C√¢u tr·∫£ l·ªùi t·ª´ chatbot
        """
        if not prompt:
            prompt = """B·∫°n l√† m·ªôt chuy√™n gia t∆∞ v·∫•n kinh t·∫ø Vi·ªát Nam. 
H√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c, c√≥ c∆° s·ªü, v√† h·ªØu √≠ch cho ng∆∞·ªùi d√πng."""

        print(f"\n{'='*60}")
        print(f"üìù C√¢u h·ªèi: {question}")
        print(f"{'='*60}")

        # Chu·∫©n b·ªã input state
        input_state = {
            "question": question,
            "generation": "",
            "documents": [],
            "prompt": prompt
        }

        # Ch·∫°y workflow
        output_state = self.compiled_workflow.invoke(input_state)

        # L·∫•y k·∫øt qu·∫£
        answer = output_state.get("generation", "‚ùå Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi.")

        print(f"\nüí≠ C√¢u tr·∫£ l·ªùi:")
        print(f"{'-'*60}")
        print(answer)
        print(f"{'-'*60}\n")

        return answer

    def interactive_chat(self):
        """
        Ch·∫ø ƒë·ªô chat t∆∞∆°ng t√°c v·ªõi ng∆∞·ªùi d√πng.
        """
        print("\n" + "="*60)
        print("ü§ñ CHATBOT KINH T·∫æ - INTERACTIVE MODE")
        print("="*60)
        print("üìå G√µ 'exit' ho·∫∑c 'quit' ƒë·ªÉ tho√°t")
        print("C√°c t√πy ch·ªçn l·ªánh:")
        print("  - /custom_prompt <text>  : ƒê·∫∑t custom prompt")
        print("  - /clear                 : X√≥a prompt v·ªÅ m·∫∑c ƒë·ªãnh")
        print("="*60 + "\n")

        custom_prompt = None

        while True:
            question = input("‚ùì Nh·∫≠p c√¢u h·ªèi: ").strip()

            if question.lower() in ["exit", "quit"]:
                print("üëã C·∫£m ∆°n b·∫°n ƒë√£ s·ª≠ d·ª•ng chatbot. T·∫°m bi·ªát!")
                break

            if question.lower().startswith("/custom_prompt"):
                custom_prompt = question.replace("/custom_prompt", "").strip()
                print(f"‚úÖ Custom prompt ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t: {custom_prompt}\n")
                continue

            if question.lower() == "/clear":
                custom_prompt = None
                print("‚úÖ Custom prompt ƒë√£ ƒë∆∞·ª£c x√≥a, quay l·∫°i m·∫∑c ƒë·ªãnh.\n")
                continue

            if not question:
                print("‚ö†Ô∏è Vui l√≤ng nh·∫≠p m·ªôt c√¢u h·ªèi h·ª£p l·ªá.\n")
                continue

            # G·ªçi assistant ƒë·ªÉ tr·∫£ l·ªùi
            self.answer_question(question, prompt=custom_prompt)


def main():
    """
    H√†m main - ƒëi·ªÉm b·∫Øt ƒë·∫ßu ch∆∞∆°ng tr√¨nh.
    """
    # C·∫•u h√¨nh m·∫∑c ƒë·ªãnh
    VECTOR_STORE_PATH = "./chroma_economy_db"  # Thay ƒë·ªïi theo ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø
    LLM_PROVIDER = "groq"  # options: gemini, groq (groq mi·ªÖn ph√≠, rate limit cao)

    # Ki·ªÉm tra xem vector store c√≥ t·ªìn t·∫°i kh√¥ng
    if not os.path.exists(VECTOR_STORE_PATH):
        print(f"‚ùå L·ªói: Vector store kh√¥ng t√¨m th·∫•y t·∫°i '{VECTOR_STORE_PATH}'")
        print(f"üí° Vui l√≤ng ch·∫°y ingestion tr∆∞·ªõc b·∫±ng: python ingestion/vector_data_builder.py")
        sys.exit(1)

    print("üöÄ ƒêang kh·ªüi t·∫°o chatbot...")

    # Kh·ªüi t·∫°o chatbot runner
    chatbot = ChatbotRunner(
        path_vector_store=VECTOR_STORE_PATH,
        llm_provider=LLM_PROVIDER
    )

    # Demo nhanh ho·∫∑c ch·∫ø ƒë·ªô interactive
    import argparse
    parser = argparse.ArgumentParser(description="Chatbot RAG Demo")
    parser.add_argument("--question", type=str, help="C√¢u h·ªèi tr·ª±c ti·∫øp (kh√¥ng c·∫ßn interactive)")
    parser.add_argument("--llm", type=str, default="gemini", help="LLM provider (gemini, openai, local, grok)")
    args = parser.parse_args()

    # N·∫øu c√≥ c√¢u h·ªèi tr·ª±c ti·∫øp
    if args.question:
        chatbot.answer_question(args.question)
    else:
        # Ch·∫ø ƒë·ªô interactive
        chatbot.interactive_chat()


if __name__ == "__main__":
    main()
